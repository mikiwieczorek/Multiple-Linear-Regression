---
title: "Multiple Linear Regression"
author: "Mikolaj Wieczorek"
date: "1/27/2020"
output: 
 md_document:
    variant: markdown_github
---

```{r}
#Load the .Rdata
library(car)
load("~/OneDrive - MNSCU/myGithub/Statistics/Regression_models/Multiple_Linear_Regression/Multiple-Linear-Regression/Data/mult.Rdata")
load("~/OneDrive - MNSCU/myGithub/Statistics/Regression_models/Multiple_Linear_Regression/Multiple-Linear-Regression/Data/Regression.Rdata")
```

# Data

The data for these sales comes from the official public records of home sales in the King County area, Washington State. The data set contains 21,606 homes that sold between May 2014 and May 2015. 
Load the train and test data sets. The description of all variables is in the data section.
```{r}
setwd("~/OneDrive - MNSCU/myGithub/Statistics/Regression_models/Multiple_Linear_Regression/Multiple-Linear-Regression/Data")
KingTest = read.csv(file = "King County Homes (test).csv")
King = read.csv("King County Homes (train).csv")
setwd("~/OneDrive - MNSCU/myGithub/Statistics/Regression_models/Multiple_Linear_Regression/Multiple-Linear-Regression")
```

```{r}
str(King)
summary(King)
```

Explore factor and ordinal variables
```{r}
#Factor variables: 
#waterfront (dummy), renovated(dummy), zipcode
#Ordinal variables:
#view (0-4), condition (1-5), grade (1-13), 
King$waterfront = as.factor(King$waterfront)
King$renovated= as.factor(King$renovated)
King$zipcode = as.factor(King$zipcode)
summary(King)
```

First, we are going to fit a base model and discuss any deficiencies.
```{r}
names(King)
#remove ID variable to only have y and x's df
King_clean = King[,-1]
```

Fit a full model:
```{r}
lm1.king = lm(price~., data = King_clean)
#summary(lm1.king)
```


Comparative boxplots to examine the relationship between categorical/ordinal predictors and a numeric response (Price)
```{r fig.height=15, fig.width=15}
par(mfrow=c(2,2))
boxplot(price~waterfront, data = King_clean, main = "Price by Waterfront")
boxplot(price~renovated, data = King_clean, main = "Price by Renovated")
boxplot(price~zipcode, data = King_clean, main = "Price by Zipcode")
```

The relationship of predictor & response is only looked at on individual basis, not in the lm1.king model. We do not know how those predictors will be behaving together in that model.

```{r}
names(King_clean)
```

Let's plot the model to look at residuals.
```{r fig.height=15, fig.width=15}
par(mfrow=c(2,2))
plot(lm1.king)  
```
Residuals vs. Fitted: we are looking whether there is a constant variance and whether there is a curvature my model is missing. We can see that the data points are not distributed with approximately similar vertical distances - the model is not homoskedastic. There also seems to be some curvature present that this model is not addressing.

Normal Quantile plot shows that the data does not follow the normal distribution.

Residuals vs Leverage: data points to the very right may have big leverage and be "pulling" (affecting) the rest of the model. Some log transformations could maybe fix this issue.



```{r eval=FALSE, include=FALSE}
vif(lm1.king)
```
We are running vif(lm1.king) function from the car package to check the variance inflation factor; the value larger than 10 would indicate that our model is facing multicollinearity concerns. We are getting an error message talking about aliased coefficients in the model. It means that there is perfect multicollinearity present. Perfect multicollinearity means that some predictors are perfectly correlated with one another (+1 or -1). These might be caused by sqft_above and sqft_basement predictors as they are perfect opposites of each other.

Extracting fitted/predicted values and then plot actual vs. predicted.
```{r}
y = King_clean$price
yhat = predict(lm1.king, data = King_clean)
ehat = resid(lm1.king)
ehat = y - yhat
```

We will not plot Actual vs. Predicted and Residuals vs. Predicted Price plots. The trendscat() function will add +/- SD bounds on the graph.
```{r fig.height=10, fig.width=10}
par(mfrow=c(1,2))
trendscat(y,yhat, xlab = "Actual Price", ylab = "Predicted Prince")
abline(0,1, lwd = 2, col = "red")
trendscat(y, ehat, xlab = "Predicted Price", ylab = "Residuals")
abline(h = 0, lwd = 2, col = "red")
```
The current model does not predict the price so well.


## Backward Elimination

We are going to use stepwise reduciton model to simplify it.
```{r}
#Backward Elimination
back.king = step(lm1.king, direction = "backward")
```

...

We can get the analysis of variance table, ANOVA, with anova() function
```{r}
anova(back.king)
```
...

Let's check what the stepwise selction eliminated:
```{r}
back.king$anova
```
<br> sqft_basement - which makes senese at it is the opposite of sqft_above and we only need one in the model.

## Mixed model selection

The mixed stepwise selection method may work better (as it uses both forward and backward propagation)
```{r}
mixed.king = step(lm1.king)
```
... discussion 

Kept predictors:
```{r}
anova(mixed.king)
```

Removed predictors:
```{r}
mixed.king$anova
```
Same as for the backward mode: sqft_basement was eliminated.

Final model: mixed.king 

When sqft_basement was elminated, VIF() funciton can be run to test for multicollinearity in the model.
```{r}
VIF(mixed.king)
```
There is multicollinearity present (especially concerned with yr_renovated and renovated).

## Using cross-validation methods to estimate the prediction error of this model using split-sample, k-fold, and the .632 bootstrap approaches

### Split-sample approach

Create a validation set: split training set onto the training and validation (70/30)

size = n; number of rows in the training data set
```{r}
n = nrow(King_clean)
CV = sample(c("Train", "Valid"), size = n, replace = T, prob = c(.70, .30))

```

```{r}
king.lm2 = lm(price~., data = King_clean[CV == "Train",])
#summary(king.lm2)
```

Mixed model stepwise: use the final model above
```{r}
mixed.king2 = step(king.lm2)
```

```{r}
mixed.king2$anova
```
Stepwise removed sqft_basement predictor again (sometimes removing sqft_lot15 as well).

mixed.king2 to use to validate:

```{r include=FALSE}
PredAcc = function(y, ypred){
  RMSEP = sqrt(mean((y-ypred)^2))
  MAE = mean(abs(y-ypred))
  MAPE = mean(abs(y-ypred)/y)*100
  cat("RMSEP\n")
  cat("================\n")
  cat(RMSEP, "\n\n")
  cat("MAE\n")
  cat("================\n")
  cat(MAE, "\n\n")
  cat("MAPE\n")
  cat("================\n")
  cat(MAPE, "\n\n")
  return(data.frame(RMSEP = RMSEP, MAE = MAE, MAPE = MAPE))
  
}

myBC = function(y) {
  require(car)
  BCtran(y)
  results = powerTransform(y)
  summary(results)
}

kfold.MLR.log = function(fit,k=10) {
  sum.sqerr = rep(0,k)
  sum.abserr = rep(0,k)
  sum.pererr = rep(0,k)
  y = fit$model[,1]
  y = exp(y)
  x = fit$model[,-1]
  data = fit$model
  n = nrow(data)
  folds = sample(1:k,nrow(data),replace=T)
  for (i in 1:k) {
    fit2 <- lm(formula(fit),data=data[folds!=i,])
    ypred = predict(fit2,newdata=data[folds==i,])
    sum.sqerr[i] = sum((y[folds==i]-ypred)^2)
    sum.abserr[i] = sum(abs(y[folds==i]-ypred))
    sum.pererr[i] = sum(abs(y[folds==i]-ypred)/y[folds==i])
  }
  cv = return(data.frame(RMSEP=sqrt(sum(sum.sqerr)/n),
                         MAE=sum(sum.abserr)/n,
                         MAPE=sum(sum.pererr)/n))
}



bootlog.cv = function(fit,B=100,data=fit$model) {
  yt=fit$fitted.values+fit$residuals
  yact = exp(yt)
  yhat = exp(fit$fitted.values)
  resids = yact - yhat
  ASR=mean(resids^2)
  AAR=mean(abs(resids))
  APE=mean(abs(resids)/yact)
  boot.sqerr=rep(0,B)
  boot.abserr=rep(0,B)
  boot.perr=rep(0,B)
  y = fit$model[,1]
  x = fit$model[,-1]
  n = nrow(data)
  for (i in 1:B) {
    sam=sample(1:n,n,replace=T)
    samind=sort(unique(sam))
    temp=lm(formula(fit),data=data[sam,])
    ytp=predict(temp,newdata=data[-samind,])
    ypred = exp(ytp)
    boot.sqerr[i]=mean((exp(y[-samind])-ypred)^2)
    boot.abserr[i]=mean(abs(exp(y[-samind])-ypred))
    boot.perr[i]=mean(abs(exp(y[-samind])-ypred)/exp(y[-samind]))
  }
  ASRo=mean(boot.sqerr)
  AARo=mean(boot.abserr)
  APEo=mean(boot.perr)
  OPsq=.632*(ASRo-ASR)
  OPab=.632*(AARo-AAR)
  OPpe=.632*(APEo-APE)
  RMSEP=sqrt(ASR+OPsq)
  MAEP=AAR+OPab
  MAPEP=(APE+OPpe)*100
  cat("RMSEP\n")
  cat("===============\n")
  cat(RMSEP,"\n\n")
  cat("MAE\n")
  cat("===============\n")
  cat(MAEP,"\n\n")
  cat("MAPE\n")
  cat("===============\n")
  cat(MAPEP,"\n\n")
  return(data.frame(RMSEP=RMSEP,MAE=MAEP,MAPE=MAPEP))  
}

```

```{r}
y = King_clean$price[CV == "Valid"]
ypred = predict(mixed.king2, newdata = King_clean[CV=="Valid",])
results = PredAcc(y,ypred)
```
RMSEP = 161013.1
MAE = 97808.4
MAPE = 20.40%

#### K-fold 

Rebuild the model:
```{r include=FALSE}
king.lm1 = lm(price~., data = King_clean)
mixed.king1 = step(king.lm1)
```

```{r}
kfold.results.full = kfold.MLR(king.lm1, k=10)
```
```{r}
kfold.results.full
```
We cannot use the full basic model as there are at least two predictors that are perfectly correlated; rank deficient means that at least one column depends on other column.

Below, we are using the simplified model by stepwise selection:

```{r}
kfold.results.step = kfold.MLR(mixed.king1, k=10)
kfold.results.step
```
RMSE is better for the step model than for the full model.

#### .632 Bootstrap

```{r include=FALSE}
king.lm1 = lm(price~., data = King_clean)
mixed.king1 = step(king.lm1)
```

```{r include=FALSE}
boot.results.full = bootols.cv(king.lm1, B=100)
```
```{r}
boot.results.full
```
```{r}
boot.results.step = bootols.cv(mixed.king1, B=100)
```
```{r}
boot.results.step
```
boot.results.step had error rates smaller MAE (97719.86) and MAPE (19.93%). RMSE (164952) was a bit higher than that of the full model (163795.6).

Compare the prediction error metrics: 

Split-sample: 
RMSE = 161013.1
MAE = 97808.4
MAPE = 20.40

K-fold:
RMSE = 164542.1
MAE = 97687.6
MAPE = 19.95%

Boot strap:
RMSE = 164952
MAE = 97719.85
MAPE = 19.93%

These will serve to compare the predicted accuracy of the model in which we are going to address the mentioned defficiencies.

# PART 2


Check for 0s or negative values
```{r}
king.lm1 = lm(price~., data = King_clean)
summary(king.lm1)
```

### Check for skewness:
```{r fig.height=25, fig.width=25}
pairs.plus2(King_clean[,c(1:6,8:14,17:20)])
```

### Transforming Home Price

Statplot() function provided in the .Rdata file can check for the distribution of the specified predictor or response.
```{r}
#Statplot(King_clean$price)
```
It is very right skewed.

Let's try logging the price and see.
```{r echo=TRUE}
#Statplot(log(King_clean$price))
```
Using log transformation on the response (price) fixes the normality concern. Now, let's see if that was the "best" transformation:

```{r include=FALSE}
function(y) {
  require(car)
  BCtran(y)
  results = powerTransform(y)
  summary(results)
}
```
```{r}
myBC(King_clean$price)
```

Since there "best transformaiton" is very complicated and logging the reponse helped fix the skewness (normality concern) and it's interpretable, we are leaving it. So, the response is being logged.


Identify predictors with very skewed distributions to see if they need transformations:

```{r}
summary(King_clean)
```

Longitude has negative values; yr_renovated, view, bathrooms, nedrooms have 0 values. Sqft_basement has also 0 values but it was removed from the model so we disregard it.

Check "best" labmda for very skewed predictors. 
```{r}
myBC(King_clean$bedrooms+1)
#labmda = 0.3 for bedrooms
```
```{r}
myBC(King_clean$bathrooms+1)
# lambda = 0.4 for bathrooms
```
```{r}
myBC(King_clean$sqft_living)
#log(sqft_living)
```

```{r}
myBC(King_clean$sqft_lot)
#lambda = -.20
```

```{r}
myBC(King_clean$floors+1)
#lambda = -.40
```

```{r}
myBC(King_clean$view+1)
#lambda = -2
```
```{r}
myBC(King_clean$condition)
#lambda = -.3
```

```{r}
myBC(King_clean$grade)
#lambda = -.3
```
```{r}
myBC(King_clean$sqft_above)
#lambda = -.2
```
```{r}
myBC(King_clean$yr_built)
#lambda = 2
```
```{r}
myBC(King_clean$yr_renovated+1)
#lambda = -2
```
```{r}
myBC(King_clean$lat)
#lambda = 2
```

Predictors to be transformed.
When transformed, check with Statplot() function.

Applying transformaitons and to the copy of King_clean data frame, called King_Trans

```{r}
#Copy of the training set
King_Trans = King_clean[,-12] #removing the sqft_basement
King_Trans$bedrooms = yjPower(King_Trans$bedrooms, 0.30)
King_Trans$bathrooms = yjPower(King_Trans$bathrooms, 0.40)
King_Trans$sqft_living = bcPower(King_Trans$sqft_living, 0)
King_Trans$sqft_lot = bcPower(King_Trans$sqft_lot, -0.20)
King_Trans$floors = yjPower(King_Trans$floors, -.40)
King_Trans$view = yjPower(King_Trans$view, -2)
#King_Trans$condition = bcPower(King_Trans$condition, -0.30)
King_Trans$grade = bcPower(King_Trans$grade, -0.30)
King_Trans$sqft_above = bcPower(King_Trans$sqft_above, -0.20)
#King_Trans$yr_built = bcPower(King_Trans$yr_built, 2)
#King_Trans$yr_renovated = yjPower(King_Trans$yr_renovated, 2)
#King_Trans$lat = bcPower(King_Trans$lat, 2)

```

```{r}
trans.lm1 = lm(price~., data = King_Trans)

```
```{r}
par(mfrow=c(2,2))
plot(trans.lm1)
```
This is not that much better at all. Let's fit a model with the price (response) transformed as well.


```{r echo=TRUE}
King_Trans$price =  log(King_Trans$price)
#Statplot(King_Trans$price)
```
The reposnse is a go. It looks way better.

```{r}
loghp.trans.lm1 = lm(price~., data = King_Trans)
par(mfrow=c(2,2))
plot(loghp.trans.lm1)

```
The residual vs. Fitted plot looks way better. Now, we have constant variance!

Let's address some multicolinearity by checking the variance inflation factor, vif(). Anything VIF > 10 would be a concern of multicollinearity.
```{r}
VIF(loghp.trans.lm1)
```
renovated & yr_renovated look like there might be some multicollinearity. Let's test a model without one:

renovated is a dummy variable telling us whether a house has been renovated at all or not; yr_renovated tells us the year a house has been last renovated (0 means that it has not been). Yr_renovated has an odd notation so I am choosing to keep renovated (dummy variable)
```{r}
names(King_Trans)
loghp.trans.lm2 = lm(price~., data = King_Trans[,-13])
par(mfrow=c(2,2))
#plot(loghp.trans.lm2)
#summary(trans.lm3)
vif(loghp.trans.lm2)

```
Now, the model looks way better.

latitude and longitude --> but it makes practical sense to leave them be in the model since latitude or longitude by themsleves would not give as all the informaiton needed (when they are both present in the model).

zipcode: we have 70 different zipcodes. 
```{r}
str(King_Trans$zipcode)
table(King_Trans$zipcode)
```
It makes sense for some zipcodes to be correlated with others. Even though each full zip code represents its own area, the zip code prefix represents a region in a given state (the are many zip codes with similar prefixes: 980, 981)

Should we include? Since we already have a latitude and longitude? Let's see how the stepwise selection sees it.

Predictive performance of loghp.trans.lm2

```{r include=FALSE}
loghp.trans.lm2.step = step(loghp.trans.lm2)
```
```{r}
loghp.trans.lm2.step$anova
```

Two of the above predictors were removed from the final model.

Let's check the metrics of this model vs. the one that did not have the response logged.
```{r}
y=King_clean$price
yloghat = fitted(loghp.trans.lm2.step)
yexp = exp(yloghat)
RMSElog = sqrt(mean((y-yexp)^2))
MAPElog = mean(abs(y-yexp)/y)
MAElog = mean(abs(y-yexp))

ypred = fitted(trans.lm1)
RMSEorig = sqrt(mean((y-ypred)^2))
MAPEorig = mean(abs(y-ypred)/y)
MAEorig = mean(abs(y-ypred))

RMSElog 
RMSEorig
```


```{r}
MAPElog
MAPEorig
#these need to be times by 100 to be in %
```


```{r}
MAElog
MAEorig
```
The metrics are much much better for the model loghp.trans.lm2.step than trans.lm1 It accounts for multicollinearity, it has constant variance, and is fixed for normality (it has applied transformations of predictors and logged response). It is also simpler because of the stepwise selection method used that removed both floors and yr_renovated from the model. It was important to also know when not to use the Box-Cox transformations on the predictors when we were looking for "optimal" lambdas - it was trying to every time suggest some lambda transformation, but with many model-checking (RMSE, MAE, MAPE) back and forth, the metrics were doing better only if the predictors which had most skewness were transformed.

The final model here seems very good. Now, we are going to cross-validate it and see how it predicts.

### Cross validate using split-sample approach ###

```{r}
CV = sample(c("Train", "Valid"), size = n, replace = T, prob = c(0.70 , 0.30))
cv.loghp.trans.lm2.step = lm(price~., data = King_Trans[CV == "Train",-13])
cv.loghp.trans.lm2.step = step(cv.loghp.trans.lm2.step)
cv.loghp.trans.lm2.step$anova
```
```{r}
summary(cv.loghp.trans.lm2.step)
vif(cv.loghp.trans.lm2.step)
```
VIF, as discussed above, remained pretty much the same for the same variables after taking out the renovated dummy variable.

Becasue of random sample splitting, when doing stepwise methods, sometimes sqft_lot15, yr_built and floors are removed or just yr_built and floors or sometimes just floors. Let's see how well the model predicts:
```{r}
y = King_Trans$price[CV == "Valid"]
y = exp(y)
ypred = predict(cv.loghp.trans.lm2.step, newdata = King_Trans[CV=="Valid",])
results = PredAcc(y,ypred)
```

###Cross validate using k-fold ###
```{r}
kfold.MLR.log(loghp.trans.lm2.step, k=10)
```


###Cross validate using .632 bootstrat approach ###
```{r}
bootlog.cv(loghp.trans.lm2.step, B=100)
```
The best predictions had the .632 bootstrap of the transformed model (logged response with predictors transformed the way it was described above).


The final model from problem 1 where we did not do any modifications or acocunt for defficiencies:
Boot strap:
RMSE = 164,952
MAE = 97,719.85
MAPE = 19.93%

THe final model after transformations
Boot strap:
RMSE = 135,906.9
MAE = 74893.33
MAPE = 13.7%
```{r}
#Review the model
anova(loghp.trans.lm2.step)
```
```{r}
#The following predictors were removed
loghp.trans.lm2.step$anova
```

The model with transformations and predictors that were removed in the stepwise selection (and sqft_basement and renovated removed because of multicollinearity) predicted the best. 

# Let's predict using the test set right now. 

First, we need to apply the same chages and transformations we did to the training set to the test set.
```{r}
KingTest$waterfront = as.factor(KingTest$waterfront)
KingTest$renovated= as.factor(KingTest$renovated)
KingTest$zipcode = as.factor(KingTest$zipcode)
```


Do the same transformations to the test set as we've done to the training set:
```{r}
names(KingTest)
#Transformations:
#Copy of the training set
King_Test = KingTest[,-1] #removin ID
King_Test = KingTest[,-12] #removing the sqft_basement
King_Test$bedrooms = yjPower(King_Test$bedrooms, 0.30)
King_Test$bathrooms = yjPower(King_Test$bathrooms, 0.40)
King_Test$sqft_living = bcPower(King_Test$sqft_living, 0)
King_Test$sqft_lot = bcPower(King_Test$sqft_lot, -0.20)
King_Test$floors = yjPower(King_Test$floors, -.40)
King_Test$view = yjPower(King_Test$view, -2)
#King_Trans$condition = bcPower(King_Trans$condition, -0.30)
King_Test$grade = bcPower(King_Test$grade, -0.30)
King_Test$sqft_above = bcPower(King_Test$sqft_above, -0.20)
#King_Trans$yr_built = bcPower(King_Trans$yr_built, 2)
#King_Test$yr_renovated = yjPower(King_Test$yr_renovated, 2)
#King_Trans$lat = bcPower(King_Trans$lat, 2)

```

# <b>Predict and Write to .csv</b>
```{r}
mypred = predict(loghp.trans.lm2.step,newdata=King_Test)
#The response is logged. Make sure to convert it back to USD!
mypred.dollars = exp(mypred)

#Save it as a data frame that also contains ID of each individual home
submission = data.frame(ID=KingTest$ID,ypred=mypred.dollars)
#Write to .csv file
write.csv(submission,file="Predictions.csv")

```


